{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18c3b4a4-1bbc-4d29-a26a-ad1a08487498",
   "metadata": {},
   "source": [
    "# Probability for Machine Learning\n",
    "-------\n",
    "# Discover How To Harness Uncertainty With Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e68ee2-bab8-4302-bf8c-26ee5a489e12",
   "metadata": {},
   "source": [
    "# Preface\n",
    "Software Developers do not have to know probablity because they are focused on deterministic programs with inputs and outputs and no random noise.\n",
    "<br>This is a problem becuase the fundamentals of predictive modeling is probability.\n",
    "<br>Practioners tend to study probability the wrong way. \n",
    "<br>They need methods that clearly state when they need to be used and how to interpret their results.\n",
    "<br>They need code they can immediately use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575175f0-47cb-4e40-92b8-f06a9506581b",
   "metadata": {},
   "source": [
    "# Part I - Introduction\n",
    "### Outcomes of this book\n",
    "- join, marginal, conditional probability\n",
    "- data is random variables in discrete, and continuous ditributions functions\n",
    "- use probabalistic methods to evaluate machine learning models directly without testing them on datasets\n",
    "- Bayesian and Naive Bayesian, Bayesian Optimization\n",
    "- qantiy uncertainty with information theory\n",
    "- naive classifiers using probablisitic framework\n",
    "- use loss functions and performance measure when training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57825aad-d0a8-4502-8fee-8055a0efc2a3",
   "metadata": {},
   "source": [
    "# Part II - Background\n",
    "-----------------\n",
    "# Chapter 1 - What is Probability?\n",
    "- Probability Theory is Mathematics about uncertainty\n",
    "\n",
    "## 1.2 Uncertainty is Normal\n",
    "Programming is based on certainty of execution deterministically\n",
    "\n",
    "Probability measure the likelihood that an event will occur.\n",
    "<br>$$\\text{probability} = \\frac{\\text{occurrences}}\n",
    "{\\text{non-occurrences} + \\text{occurrences}}$$\n",
    "This always is a value between **1 and 0**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564237c2-7235-4f2a-bdbc-20b95bcee922",
   "metadata": {},
   "source": [
    "Probability (usually represented as lowercase *p*) of throwing d6 is 1/6 or 0.166 \n",
    "<br> Percent is *p* multiplied by 100 to give us 16.6%\n",
    "<br> Odds are written wins:losses like 1:6\n",
    "$$\\text{P(flood)} = {\\text{probability of flood}}$$\n",
    "$$\\text{1 - P(flood)} = {\\text{probability of no flood}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da424c-da8d-48b9-b402-c636e4531d91",
   "metadata": {},
   "source": [
    "Probability is logic that can be used to manage uncertainty.\n",
    "<br>And provides formal rules for determining likelihood of a proposition\n",
    "<br> 3 concepts:\n",
    "\n",
    "- **Event** (A) An outcome which a probability is assigned\n",
    "- **Sample Space** (S) Set of possible outcomes/events\n",
    "- **Probability Function** (P) used to assign probability to an outcome/event\n",
    "\n",
    "The likelihood of (A) drawn from the sample space (S) is determined by (P).\n",
    "The shape of distribution of all events is the Probability Disribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131602c4-8913-4dff-8398-4e761718e89c",
   "metadata": {},
   "source": [
    "### 1.5 Two School of Probability\n",
    "- **Frequestist Probability:** frequency of events provide basis for calculating probability. Inlcude p-values and confidence intervals from inferential statistics and maximum likelihood estimation for parameter estimation.\n",
    "- **Bayesian Probability:** probabilities are assigned based on evidence and personal belief and Bayes' theorem and allows for less frequent or unlikely events to be predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aaed6f-fc22-494d-8dae-0525419f60df",
   "metadata": {},
   "source": [
    "1.6.1 Books\n",
    "- [Probability Theory: The Logic of Science, 2003.](https://amzn.to/2lnW2pp)\n",
    "- Introduction to Probability, 2nd edition, 2019. https://amzn.to/2xPvobK\n",
    "- Introduction to Probability, 2nd edition, 2008. https://amzn.to/2llA3PR\n",
    "1.6.2 Articles\n",
    "- Uncertainty, Wikipedia. https://en.wikipedia.org/wiki/Uncertainty\n",
    "- Probability, Wikipedia. https://en.wikipedia.org/wiki/Probability\n",
    "- Odds, Wikipedia. https://en.wikipedia.org/wiki/Odds\n",
    "- Probability theory, Wikipedia. https://en.wikipedia.org/wiki/Probability_theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7b1e0a-8360-4580-995a-35f3d68c5605",
   "metadata": {},
   "source": [
    "-------------\n",
    "# Chapter 2 - Uncertainty in Machine Learning\n",
    "- Uncertainty is the biggest source of difficulty\n",
    "- **1)** Noise in data, \n",
    "- **2)** incomplete coverage, \n",
    "- **3)** imperfect models\n",
    "- Probability allows us to work with uncertainty\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b776dbf8-c26d-47e0-9c52-0228b6e693e3",
   "metadata": {},
   "source": [
    "- What are the best features?\n",
    "- What is the best algorithm?\n",
    "\n",
    "<br>These cannot be known exactly and need to systematically evaluated until a good enough answer arises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b282ccef-427e-44c0-b5b4-3ea44cfeae5c",
   "metadata": {},
   "source": [
    "## Noise in Observation\n",
    "Noise refers to variablity in the observation which could be natural or an error or randomness.\n",
    "## Incomplete Coverage of Domain\n",
    "The Population is never 100% complete and will always by a limited sample of observations.\n",
    "<br> What matters is that an apporpriate level of variance is present. \n",
    "## Imperfect Model of the Problem\n",
    "All models are wrong but some are useful. Because of Noise and Incompleteness we cannot avoid this error.\n",
    "<br>We can simply use robust but simpler models instead of highly specialised and complex models.\n",
    "<br>We even use models that produce errors in order to generalise in later stages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a61681-bc97-4a22-83da-cc659d218ecd",
   "metadata": {},
   "source": [
    "In many cases, it is more practical to use a simple but uncertain rule rather than a complex but certain one, even if the true rule is deterministic and our modeling system has the fidelity to accommodate a complex rule.\n",
    "â€” Page 55, Deep Learning, 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673db853-b6b1-4606-9567-742b3246a49f",
   "metadata": {},
   "source": [
    "## Managing Uncertainty\n",
    "- **Noisy Observtions** probability and statistics hel us understand and quality the expected variablitiy in our obsevations from the domain\n",
    "- **Incompleteness of COverage** probabitlity helps us show the distribution and density of obsevation in the domain\n",
    "- **Model Error** probabiltiy helps us show expected capability and variance in our predictive models on new data\n",
    "\n",
    "<br>Machine learning models also called maximum likelihood estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d41b0ec-0166-40da-aabe-b7e12cfe35cd",
   "metadata": {},
   "source": [
    "2.7.1 Books\n",
    "- Chapter 3: Probability Theory, Deep Learning, 2016. https://amzn.to/2lnc3vL\n",
    "- Chapter 2: Probability, Machine Learning: A Probabilistic Perspective, 2012. https://amzn.to/2xKSTCP\n",
    "- Chapter 2: Probability Distributions, Pattern Recognition and Machine Learning, 2006. https://amzn.to/2JwHE7I\n",
    "2.7.2 Articles\n",
    "- Uncertainty, Wikipedia. https://en.wikipedia.org/wiki/Uncertainty\n",
    "- Stochastic, Wikipedia. https://en.wikipedia.org/wiki/Stochastic\n",
    "- All models are wrong, Wikipedia. https://en.wikipedia.org/wiki/All_models_are_wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd64019-3d6a-4479-9c8b-dc7cf131e7dc",
   "metadata": {},
   "source": [
    "--------\n",
    "# Chapter 3 - Why Learn Probability for Machine Learning\n",
    "- not everyone should learn probability \n",
    "<br>Because:\n",
    "1) It is not required to use machine learning as a tool\n",
    "2) It's slow taking months or years before getting your hands on ML and will delay your goal.\n",
    "3) Its a huge field and not all of it is relevant to ML\n",
    "\n",
    "<br>Results first approach to ML is better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d966f07-4373-4126-b3c5-03fe639da4aa",
   "metadata": {},
   "source": [
    "### Class Membership (predicting a probability)\n",
    "\n",
    "<u>**Classification**</u> predictive modeling is where an example is assigned a given probable label.\n",
    "<br> Probabilities can be scaled or transformed using the probability calibration process\n",
    "\n",
    "### Some algorithms are designed with Probability\n",
    "\n",
    "- Naive Bayes\n",
    "- Probabilistic Graphical Models (PGM)\n",
    "- Bayesian Belief Networks (Bayes Nets) capture conditional dependencies bewteen variables\n",
    "\n",
    "### Models are Trained using a Probablistic Framework\n",
    "The framework of MLE (maximum likelihood estimation) which estimates model parameters (wieghts) of observed data.\n",
    "<br> This MLE underlies the Ordinary Least Squares estimate of Linear Regression.\n",
    "<br> The Expectation-Maximization algorithm  or (EM) is often used for unsupervised data clustering, k means for k clusters (k-Means clustering)\n",
    "<br> Entropy, the differences between distributions measured via KL divergence, is the calculated directly as the negative log of the probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ebdc98-703a-4041-b6a0-59bd50df699a",
   "metadata": {},
   "source": [
    "### Models can be tuned with Proabalistic Framework\n",
    "Hyperparameter tuning is finding the optimal values of hyperparameters of the models to improve performance.\n",
    "<br> Bayesian  Optimizations is a more effiecient hyperparameter optimization\n",
    "\n",
    "### Probablisic Measures are used to Evaluate Model Skill\n",
    "evaluation measures are required to sumarize the performance of a model\n",
    "\n",
    "\n",
    "- log loss\n",
    "- Brier score\n",
    "- Reciever Operating Characteristic (ROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d0dc24-3cab-42ea-80ab-cdf446529cfe",
   "metadata": {},
   "source": [
    "3.9.1 Books\n",
    "- Pattern Recognition and Machine Learning, 2006. https://amzn.to/2JwHE7I\n",
    "- Machine Learning: A Probabilistic Perspective, 2012. https://amzn.to/2xKSTCP\n",
    "- Machine Learning, 1997. https://amzn.to/2jWd51p\n",
    "3.9.2 Articles\n",
    "- Graphical model, Wikipedia. https://en.wikipedia.org/wiki/Graphical_model\n",
    "- Maximum likelihood estimation, Wikipedia. https://en.wikipedia.org/wiki/Maximum_likelihood_estimation\n",
    "- Expectation-maximization algorithm, Wikipedia. https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm\n",
    "- Cross entropy, Wikipedia. https://en.wikipedia.org/wiki/Cross_entropy\n",
    "- Kullback-Leibler divergence, Wikipedia. https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
    "- Bayesian optimization, Wikipedia. https://en.wikipedia.org/wiki/Bayesian_optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e8c707-320a-40b3-ad2f-14a0a2e56f96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_mastery",
   "language": "python",
   "name": "machine_learning_mastery"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
